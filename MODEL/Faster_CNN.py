# -*- coding: utf-8 -*-
"""Untitled6.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1W24lxgOL0fkYIXFbusVat50mcxub_NhH
"""

# Install the necessary components
!pip install pyyaml==5.1 --quiet  # Installation of pyyaml 5.1

import torch  # Importing PyTorch library

# Retrieve the version of PyTorch and CUDA
TORCH_VERSION = ".".join(torch.__version__.split(".")[:2])
CUDA_VERSION = torch.__version__.split("+")[-1]
print("torch: ", TORCH_VERSION, "; cuda: ", CUDA_VERSION)  # Displaying versions

# Install Detectron2 from GitHub repository
!python -m pip install 'git+https://github.com/facebookresearch/detectron2.git' --quiet  # Installing Detectron2

# exit(0)  # Restart runtime if needed

"""IMPORTING NECESSARY LIBRARIES"""

# Basic setup
from detectron2.utils.logger import setup_logger  # Setup detectron2 logger
setup_logger()

# Import common libraries
import numpy as np
import pandas as pd
import os, random, glob
import io
import time
import datetime
import logging
import json
import torch
import cv2

# Import visualization libraries
import matplotlib.pyplot as plt
import seaborn as sns
from google.colab.patches import cv2_imshow

# Import additional libraries
from sklearn.metrics import confusion_matrix
from torch.utils.tensorboard import SummaryWriter
from tqdm import tqdm
import albumentations as A
from pycocotools import mask as maskUtils

# Import detectron2 utilities
import detectron2.utils.comm as comm
from detectron2 import model_zoo
from detectron2.engine import DefaultPredictor, DefaultTrainer, HookBase
from detectron2.config import get_cfg
from detectron2.utils.logger import log_every_n_seconds
from detectron2.utils.visualizer import Visualizer, ColorMode
from detectron2.data import MetadataCatalog, DatasetCatalog, DatasetMapper, build_detection_test_loader
from detectron2.data.datasets import register_coco_instances
from detectron2.evaluation import COCOEvaluator, inference_on_dataset
from detectron2.structures import Boxes, pairwise_iou

"""Downloading and Extracting the Dataset from Roboflow"""

# Download the dataset and unzip it
!curl -L "https://app.roboflow.com/ds/yjGLZlonKb?key=1qQLR6j7yT" > roboflow.zip; unzip roboflow.zip; rm roboflow.zip  # Download and extract dataset

"""Reading Categories"""

def check_category_names(path):
# Load the annotations file
  with open(path) as f:
      annotations = json.load(f)

  # Extract the categories
  categories = annotations['categories']

  # Display the category ids and names
  for category in categories:
      print(f"Category ID: {category['id']}, Name: {category['name']}")

check_category_names("/content/test/_annotations.coco.json")

folders = ["/content/test", "/content/train", "/content/valid"]

file_name = "_annotations.coco.json"

def update_json_file(file_path):
    with open(file_path, 'r') as f:
        data = json.load(f)

    data['categories'] = [category for category in data['categories'] if category['id'] != 0]

    data['annotations'] = [annotation for annotation in data['annotations'] if annotation['category_id'] != 0]

    data['categories'].sort(key=lambda x: x['id'])

    with open(file_path, 'w') as f:
        json.dump(data, f, indent=4)

for folder in folders:
    file_path = os.path.join(folder, file_name)
    update_json_file(file_path)
    print(f"Updated file: {file_path}")

print("All files updated.")

check_category_names("/content/test/_annotations.coco.json")

"""Registering this data as a COCO instance for use in Detectron2."""

register_coco_instances("my_dataset_train", {}, "/content/train/_annotations.coco.json", "/content/train")  # Register training dataset
register_coco_instances("my_dataset_test", {}, "/content/test/_annotations.coco.json", "/content/test")  # Register testing dataset
register_coco_instances("my_dataset_valid", {}, "/content/valid/_annotations.coco.json", "/content/valid")  # Register validation dataset

# Visualize training data
my_dataset_train_metadata = MetadataCatalog.get("my_dataset_train")
dataset_dicts = DatasetCatalog.get("my_dataset_train")

# Display random samples from the training dataset
for d in random.sample(dataset_dicts, 3):
    img = cv2.imread(d["file_name"])
    visualizer = Visualizer(img[:, :, ::-1], metadata=my_dataset_train_metadata, scale=0.5)
    vis = visualizer.draw_dataset_dict(d)
    cv2_imshow(vis.get_image()[:, :, ::-1])  # Show visualized image

